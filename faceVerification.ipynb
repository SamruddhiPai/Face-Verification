{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2P2S2_verification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-uBiD2mYQj6"
      },
      "source": [
        "#These libraries help to interact with the operating system and the runtime environment respectively\n",
        "import os\n",
        "import sys\n",
        "\n",
        "#Model/Training related libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "\n",
        "#Dataloader libraries\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.metrics import accuracy_score, multilabel_confusion_matrix\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torchvision   \n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import Compose \n",
        "from torchvision import transforms\n",
        "\n",
        "from torchsummary import summary\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SbkeYFGiZRr"
      },
      "source": [
        "#Intall Kaggle API and create kaggle directory\n",
        "!pip install --upgrade --force-reinstall --no-deps kaggle\n",
        "!mkdir .kaggle\n",
        "#This data is used to login  into your Kaggle account\n",
        "import json\n",
        "token = {\"username\":\"samruddhi98\",\"key\":\"db26269bc2e5ae4d4f8456490e50b5a8\"}\n",
        "with open('/content/.kaggle/kaggle.json', 'w') as file:\n",
        "    json.dump(token, file)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5FR5rUoidqM"
      },
      "source": [
        "!chmod 600 /content/.kaggle/kaggle.json\n",
        "\n",
        "!cp /content/.kaggle/kaggle.json /root/.kaggle/\n",
        "!kaggle config set -n path -v /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj5dEubziiuo",
        "outputId": "422b628e-0bf3-4b45-b87a-e17fe27e4054"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lDkUhvxim3M",
        "outputId": "457abd4b-702a-442b-aba9-ccaa198ef93b"
      },
      "source": [
        "!kaggle competitions download -c idl-fall21-hw2p2s2-face-verification-slack\n",
        "# !kaggle competitions download -c idl-fall21-hw2p2s1-face-classification"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading idl-fall21-hw2p2s2-face-verification-slack.zip to /content/competitions/idl-fall21-hw2p2s2-face-verification-slack\n",
            " 90% 140M/156M [00:00<00:00, 192MB/s]\n",
            "100% 156M/156M [00:00<00:00, 174MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxPBtkqeivaJ"
      },
      "source": [
        "!unzip competitions/idl-fall21-hw2p2s2-face-verification-slack/idl-fall21-hw2p2s2-face-verification-slack.zip\n",
        "# !unzip competitions/idl-fall21-hw2p2s1-face-classification/idl-fall21-hw2p2s1-face-classification.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vmQsmICsBID"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        # self.target_list = target_list\n",
        "        # self.n_class = len(list(set(target_list)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i1 = Image.open('verification_data/' + self.data.iloc[index]['img1'])\n",
        "        i1 = torchvision.transforms.ToTensor()(i1)\n",
        "        i1 = torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(i1)\n",
        "        i2 = Image.open('verification_data/' + self.data.iloc[index]['img2'])\n",
        "        i2 = torchvision.transforms.ToTensor()(i2)\n",
        "        i2 = torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(i2)\n",
        "        label = self.data.iloc[index]['label']\n",
        "        return i1, i2, label\n",
        "\n",
        "class ImageDataset_test(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "        # self.target_list = target_list\n",
        "        # self.n_class = len(list(set(target_list)))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        i1 = Image.open('verification_data/' + self.data.iloc[index]['img1'])\n",
        "        i1 = torchvision.transforms.ToTensor()(i1)\n",
        "        i1 = torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(i1)\n",
        "        i2 = Image.open('verification_data/' + self.data.iloc[index]['img2'])\n",
        "        i2 = torchvision.transforms.ToTensor()(i2)\n",
        "        i2 = torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])(i2)\n",
        "        # label = self.data.iloc[index]['label']\n",
        "        return i1, i2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm4bbBBHX1QW"
      },
      "source": [
        "####### Resnet 18-34 ##########\n",
        "\n",
        "class SimpleResidualBlock_resnet18(nn.Module):\n",
        "    def __init__(self, in_channel_size, stride=1):\n",
        "        super().__init__()\n",
        "        out_channel_size = int(in_channel_size*stride)\n",
        "        self.conv1 = nn.Conv2d(in_channel_size, out_channel_size, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(out_channel_size, out_channel_size, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channel_size)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channel_size)\n",
        "        if stride == 1:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "            self.shortcut = nn.Conv2d(in_channel_size, out_channel_size, kernel_size=1, stride=stride, bias=False)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        \n",
        "        shortcut = self.shortcut(x)\n",
        "        out = self.relu(out + shortcut)\n",
        "        return out\n",
        "    \n",
        "# This has hard-coded hidden feature sizes.\n",
        "# You can extend this to take in a list of hidden sizes as argument if you want.\n",
        "class ClassificationNetwork18(nn.Module):\n",
        "    def __init__(self, in_features, num_classes, feat_dim = 1000):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.layers_34 = nn.Sequential(\n",
        "            # nn.Conv2d(in_features, 32, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            # nn.BatchNorm2d(32),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_features, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # SimpleResidualBlock_resnet(64),\n",
        "            SimpleResidualBlock_resnet18(64),\n",
        "            SimpleResidualBlock_resnet18(64),\n",
        "            SimpleResidualBlock_resnet18(64,2),\n",
        "            SimpleResidualBlock_resnet18(128),\n",
        "            SimpleResidualBlock_resnet18(128),\n",
        "            SimpleResidualBlock_resnet18(128),\n",
        "            SimpleResidualBlock_resnet18(128,2),\n",
        "            SimpleResidualBlock_resnet18(256),\n",
        "            SimpleResidualBlock_resnet18(256),\n",
        "            SimpleResidualBlock_resnet18(256),\n",
        "            SimpleResidualBlock_resnet18(256),\n",
        "            SimpleResidualBlock_resnet18(256),\n",
        "            SimpleResidualBlock_resnet18(256,2),\n",
        "            SimpleResidualBlock_resnet18(512),\n",
        "            SimpleResidualBlock_resnet18(512),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), # For each channel, collapses (averages) the entire feature map (height & width) to 1x1\n",
        "            nn.Flatten(), # the above ends up with batch_size x 64 x 1 x 1, flatten to batch_size x 64\n",
        "        )\n",
        "\n",
        "        self.layers_18 = nn.Sequential(\n",
        "            nn.Conv2d(in_features, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            SimpleResidualBlock_resnet18(64),\n",
        "            SimpleResidualBlock_resnet18(64,2),\n",
        "            SimpleResidualBlock_resnet18(128),\n",
        "            SimpleResidualBlock_resnet18(128,2),\n",
        "            SimpleResidualBlock_resnet18(256),\n",
        "            SimpleResidualBlock_resnet18(256,2),\n",
        "            SimpleResidualBlock_resnet18(512),\n",
        "            # SimpleResidualBlock_resnet(512),\n",
        "            nn.AdaptiveAvgPool2d((1, 1)), # For each channel, collapses (averages) the entire feature map (height & width) to 1x1\n",
        "            nn.Flatten(), # the above ends up with batch_size x 64 x 1 x 1, flatten to batch_size x 64\n",
        "        )\n",
        "\n",
        "        self.linear = nn.Linear(1024, feat_dim)\n",
        "        nn.init.kaiming_normal(self.linear.weight)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.dropout = nn.Dropout(0.25)\n",
        "        self.linear_output = nn.Linear(1024,num_classes)  \n",
        "        nn.init.kaiming_normal(self.linear_output.weight)   \n",
        "\n",
        "\n",
        "    def forward(self, x, return_embedding=False):\n",
        "        embedding_34 = self.layers_34(x) \n",
        "        embedding_18 = self.layers_18(x) \n",
        "        embedding = torch.cat((embedding_34,embedding_18),1)\n",
        "        embedding_out = self.relu(self.linear(embedding))\n",
        "        output = self.linear_output(self.dropout(embedding))\n",
        "        if return_embedding:\n",
        "            return embedding_18,output\n",
        "        else:\n",
        "            return output   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoCjizSGoUZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1b263b9-0c24-4f30-b00b-d11f1f2c1fe4"
      },
      "source": [
        "closs_weight = 0.1\n",
        "lr_cent = 0.05\n",
        "feat_dim = 512\n",
        "# del network\n",
        "in_features = 3\n",
        "num_classes = 4000\n",
        "learningRate = 0.01\n",
        "numEpochs = 100\n",
        "weightDecay = 1e-4\n",
        "compute_sim = nn.CosineSimilarity(dim=1)\n",
        "network_18 = ClassificationNetwork18(in_features, num_classes)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:78: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iro1v6ACNPx8",
        "outputId": "1aa81b52-6ad0-45ed-dd93-7ed4ab032475"
      },
      "source": [
        "# load model\n",
        "\n",
        "#### Actual path used for loading various models ###\n",
        "# epoch = 42 #- gave 83%\n",
        "# val_acc = 0.8145 #0.7597\n",
        "# PATH = 'drive/MyDrive/IDL-HW2P2-S1/model-5-CL-SCE_epoch%d_val_acc-%0.04f.pt'%(epoch,val_acc)\n",
        "# PATH = 'drive/MyDrive/IDL-HW2P2-S2/ensemble_18_34_epoch%d_val_acc-%0.4f.pt'%(epoch,val_acc)\n",
        "# PATH = 'drive/MyDrive/IDL-HW2P2-S1/model6_Resnet34_epoch%d_val_acc-%0.4f.pt'%(epoch,val_acc)\n",
        "####################################################\n",
        "\n",
        "PATH = 'ensemble_model.pt'\n",
        "print('Loading %s model '%(PATH))\n",
        "checkpoint = torch.load(PATH, map_location=torch.device('cpu'))\n",
        "network_18.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading drive/MyDrive/IDL-HW2P2-S2/ensemble_18_34_epoch42_val_acc-0.8145.pt model \n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTHD9C_xY7k3"
      },
      "source": [
        "data = pd.read_csv('verification_pairs_val.txt',sep=' ', names=['img1','img2','label'])\n",
        "data_test = pd.read_csv('verification_pairs_test.txt',sep=' ', names=['img1','img2'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNK8g70cxjoY",
        "outputId": "4b78321f-c7f3-42e3-e6ac-e5b73dff961d"
      },
      "source": [
        "# dataloader\n",
        "val_dataset = ImageDataset(data)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, \n",
        "                                               shuffle=False, num_workers=4)\n",
        "test_dataset = ImageDataset_test(data_test)\n",
        "print(len(test_dataset))\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=32, \n",
        "                                               shuffle=False, num_workers=8)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "51835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jg8cebsb0n1u",
        "outputId": "fafb8985-dbb9-4d98-86aa-3138e53b70aa"
      },
      "source": [
        "# Validate\n",
        "# network_34.eval()\n",
        "network_18.to(device)\n",
        "network_18.eval()\n",
        "label_list = []\n",
        "sim_score = []\n",
        "print('Length of validation dataloader is: ',len(val_dataloader))\n",
        "\n",
        "for batch_num, (image1, image2, y) in enumerate(val_dataloader):\n",
        "    image1, image2 = image1.to(device), image2.to(device)\n",
        "    label_list.extend(list(y.numpy()))\n",
        "    emb1_18,o21 = network_18(image1,return_embedding=True)\n",
        "    emb2_18,o22 = network_18(image2,return_embedding=True)\n",
        "    similarity = compute_sim(emb1_18, emb2_18).cpu().detach().numpy()\n",
        "    sim_score.extend(list(similarity))\n",
        "    if batch_num%100==99:\n",
        "        print('Processed %d batches'%(batch_num+1))\n",
        "auc = roc_auc_score(label_list,sim_score)\n",
        "print('Validation AUC is: {:4f}'.format(auc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of validation dataloader is:  276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 100 batches\n",
            "Processed 200 batches\n",
            "Validation AUC is: 0.926283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd-uJsKMQsi9"
      },
      "source": [
        "# Test\n",
        "network_18.to(device)\n",
        "network_18.eval()\n",
        "network_res34.eval()\n",
        "label_list = []\n",
        "sim_score = []\n",
        "print('Length of validation dataloader is: ',len(test_dataloader))\n",
        "\n",
        "for batch_num, (image1, image2) in enumerate(test_dataloader):\n",
        "    image1, image2 = image1.to(device), image2.to(device)\n",
        "\n",
        "    emb1_18,o21 = network_18(image1,return_embedding=True)\n",
        "    emb2_18,o22 = network_18(image2,return_embedding=True)\n",
        "    similarity = compute_sim(emb1_18, emb2_18).cpu().detach().numpy()\n",
        "    sim_score.extend(list(similarity))\n",
        "\n",
        "    #### Tried ensemble method ####\n",
        "    # emb1_18,out = network_18(image1,return_embedding=True)\n",
        "    # emb2_18,out = network_18(image2,return_embedding=True)\n",
        "    # emb1 = torch.cat((0.5*emb1_34,1*emb1_18),1)\n",
        "    # emb2 = torch.cat((0.5*emb2_34,1*emb2_18),1)\n",
        "    # similarity = compute_sim(emb1, emb2).cpu().detach().numpy()\n",
        "    ################################\n",
        "  \n",
        "    if batch_num%100==99:\n",
        "        print('Processed %d batches'%(batch_num+1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO6rc0IT_hby",
        "outputId": "5cc5d627-0554-4bdc-db51-1738ec6eaff2"
      },
      "source": [
        "# creating the submission file\n",
        "\n",
        "sub_df = pd.read_csv('verification_pairs_test.txt', names=['Id'])\n",
        "sub_df['Category'] = sim_score\n",
        "filename = 'submissions_res34-18_epoch42-30.csv'\n",
        "sub_df.to_csv(filename,index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                      Id  Category\n",
            "0      verification_data/00020839.jpg verification_da...  0.708999\n",
            "1      verification_data/00002921.jpg verification_da...  0.454112\n",
            "2      verification_data/00011732.jpg verification_da...  0.322784\n",
            "3      verification_data/00052778.jpg verification_da...  0.560583\n",
            "4      verification_data/00053948.jpg verification_da...  0.590733\n",
            "...                                                  ...       ...\n",
            "51830  verification_data/00041961.jpg verification_da...  0.372656\n",
            "51831  verification_data/00060107.jpg verification_da...  0.338923\n",
            "51832  verification_data/00003205.jpg verification_da...  0.551374\n",
            "51833  verification_data/00068054.jpg verification_da...  0.655628\n",
            "51834  verification_data/00046783.jpg verification_da...  0.407868\n",
            "\n",
            "[51835 rows x 2 columns] 51835\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7VptiwIAI6U",
        "outputId": "5a43fa8a-8e65-474d-a473-d4b6569dfa1c"
      },
      "source": [
        "# Submitting to kaggle\n",
        "!kaggle competitions submit -c 'idl-fall21-hw2p2s2-face-verification-slack' -f {filename} -m \"submission-res34-18_epoch42\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100% 4.01M/4.01M [00:00<00:00, 17.7MB/s]\n",
            "Successfully submitted to IDL-Fall21-HW2P2S2-Face Verification-Slack"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2Y90XdzAuzS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}